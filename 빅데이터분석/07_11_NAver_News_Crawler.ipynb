{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from korea_news_crawler.articlecrawler import ArticleCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_year': 2020, 'start_month': 1, 'end_year': 2020, 'end_month': 4}\n"
     ]
    }
   ],
   "source": [
    "Crawler = ArticleCrawler()  \n",
    "Crawler.set_category(\"정치\", \"IT과학\", \"economy\")  \n",
    "Crawler.set_date_range(2020, 1, 2020, 4)  \n",
    "Crawler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019-01-01', '2019-01-02', '2019-01-03', '2019-01-04', '2019-01-05', '2019-01-06', '2019-01-07', '2019-01-08', '2019-01-09', '2019-01-10', '2019-01-11', '2019-01-12', '2019-01-13', '2019-01-14', '2019-01-15', '2019-01-16', '2019-01-17', '2019-01-18', '2019-01-19', '2019-01-20', '2019-01-21', '2019-01-22', '2019-01-23', '2019-01-24', '2019-01-25', '2019-01-26', '2019-01-27', '2019-01-28', '2019-01-29', '2019-01-30']\n"
     ]
    }
   ],
   "source": [
    "# 가져올 범위를 정의\n",
    "# 2015-02-25 ~ 2015-02-28 // 2015-03-01 ~ 2015-03-30\n",
    "\n",
    "import datetime\n",
    "\n",
    "days_range = []\n",
    "\n",
    "start = datetime.datetime.strptime(\"2019-01-01\", \"%Y-%m-%d\")\n",
    "end = datetime.datetime.strptime(\"2019-01-31\", \"%Y-%m-%d\") # 범위 + 1\n",
    "date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\n",
    "\n",
    "for date in date_generated:\n",
    "    days_range.append(date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "print(days_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_bs_obj(url):\n",
    "    result = requests.get(url)\n",
    "    bs_obj = BeautifulSoup(result.content, \"html.parser\")\n",
    "    \n",
    "    return bs_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2dd2395051489fbbde14e2e9319e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-27fbfe926963>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# 포토 뉴스 페이지 수 구하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mphoto_news_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"eh_page\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mphoto_news_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphoto_news_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "test = [\"2015-03-01\"] # 테스트를 위한 데이터 수집 구간\n",
    "\n",
    "for date in tqdm_notebook(test):\n",
    "    \n",
    "    news_arrange_url = \"https://news.naver.com/main/history/mainnews/list.nhn\"\n",
    "    news_list_date_page_url = news_arrange_url + \"?date=\" + date\n",
    "\n",
    "    # get bs_obj\n",
    "    bs_obj = get_bs_obj(news_list_date_page_url)\n",
    "    \n",
    "    # 포토 뉴스 페이지 수 구하기\n",
    "    photo_news_count = bs_obj.find(\"div\", {\"class\": \"eh_page\"}).text.split('/')[1]\n",
    "    photo_news_count = int(photo_news_count)\n",
    "    \n",
    "    print(photo_news_count)\n",
    "    \n",
    "    # 리스트 뉴스 페이지 수 구하기\n",
    "    text_news_count = bs_obj.find(\"div\", {\"class\": \"mtype_list_wide\"}).find(\"div\", {\"class\": \"eh_page\"}).text.split('/')[1]\n",
    "    text_news_count = int(text_news_count)\n",
    "    \n",
    "    print(text_news_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206f365d8cf24c8793f0cf81b0b2c6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c334ecfb901c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# 포토 뉴스 페이지 수 구하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mphoto_news_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"eh_page\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mphoto_news_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphoto_news_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# 가져올 범위를 정의\n",
    "# 2015-02-25 ~ 2015-02-28 // 2015-03-01 ~ 2015-03-31\n",
    "\n",
    "import datetime\n",
    "\n",
    "days_range = []\n",
    "\n",
    "start = datetime.datetime.strptime(\"2015-02-25\", \"%Y-%m-%d\") # 수집 시작 날짜\n",
    "end = datetime.datetime.strptime(\"2015-03-31\", \"%Y-%m-%d\") # 수집 종료 날짜 + 1\n",
    "date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\n",
    "\n",
    "for date in date_generated:\n",
    "    days_range.append(date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# 크롤러 작성\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "main_news_list = []\n",
    "\n",
    "# html parser 정의\n",
    "def get_bs_obj(url):\n",
    "    result = requests.get(url)\n",
    "    bs_obj = BeautifulSoup(result.content, \"html.parser\")\n",
    "    \n",
    "    return bs_obj\n",
    "    \n",
    "for date in tqdm_notebook(days_range):\n",
    "    \n",
    "    news_arrange_url = \"https://news.naver.com/main/history/mainnews/list.nhn\"\n",
    "    news_list_date_page_url = news_arrange_url + \"?date=\" + date\n",
    "\n",
    "    # get bs_obj\n",
    "    bs_obj = get_bs_obj(news_list_date_page_url)\n",
    "    \n",
    "    # 포토 뉴스 페이지 수 구하기\n",
    "    photo_news_count = bs_obj.find(\"div\", {\"class\": \"eh_page\"}).text.split('/')[1]\n",
    "    photo_news_count = int(photo_news_count)\n",
    "    \n",
    "    # 리스트 뉴스 페이지 수 구하기\n",
    "    text_news_count = bs_obj.find(\"div\", {\"class\": \"mtype_list_wide\"}).find(\"div\", {\"class\": \"eh_page\"}).text.split('/')[1]\n",
    "    text_news_count = int(text_news_count)\n",
    "    \n",
    "    # 포토 뉴스 부분 링크 크롤링\n",
    "    for page in tqdm_notebook(range(1,photo_news_count+1)):\n",
    "        \n",
    "        # 포토 뉴스 링크\n",
    "        news_list_photo_url = 'http://news.naver.com/main/history/mainnews/photoTv.nhn'\n",
    "        date_str = \"?date=\"\n",
    "        page_str = \"&page=\"\n",
    "        news_list_photo_full_url = news_list_photo_url + \"?date=\" + date + \"&page=\" + str(page)\n",
    "        \n",
    "        # get bs obj\n",
    "        photo_bs_obj = get_bs_obj(news_list_photo_full_url)\n",
    "        \n",
    "        # 링크 내 정보 수집\n",
    "        ul = photo_bs_obj.find(\"ul\", {\"class\": \"edit_history_lst\"})\n",
    "        lis = ul.find_all(\"li\")\n",
    "        for item in lis:\n",
    "            title = item.find(\"a\")[\"title\"]\n",
    "            press = item.find(\"span\", {\"class\" : \"eh_by\"}).text\n",
    "            \n",
    "            # link\n",
    "            link = item.find(\"a\")[\"href\"]\n",
    "            \n",
    "            sid1 = link.split('&')[-3].split('=')[1]\n",
    "            oid = link.split('&')[-2].split('=')[1]\n",
    "            aid = link.split('&')[-1].split('=')[1]            \n",
    "            \n",
    "            # 연예 TV 기사 제외\n",
    "            if sid1 == \"shm\":\n",
    "                continue\n",
    "                \n",
    "            article_type = \"pic\"\n",
    "            \n",
    "            pic_list = [date, article_type, title, press, sid1, link, aid]\n",
    "            \n",
    "            main_news_list.append(pic_list)\n",
    " \n",
    "    # 텍스트 뉴스 부분 링크 크롤링\n",
    "    for page in tqdm_notebook(range(1, text_news_count+1)):\n",
    "        \n",
    "        # 텍스트 뉴스 링크\n",
    "        news_list_text_url = 'http://news.naver.com/main/history/mainnews/text.nhn'\n",
    "        date_str = \"?date=\"\n",
    "        page_str = \"&page=\"\n",
    "        news_list_text_full_url = news_list_text_url + \"?date=\" + date + \"&page=\" + str(page)\n",
    "        \n",
    "        # get bs obj\n",
    "        text_bs_obj = get_bs_obj(news_list_text_full_url)\n",
    "        \n",
    "        # 링크 내 정보 수집\n",
    "        uls = text_bs_obj.find_all(\"ul\")\n",
    "        for ul in uls:\n",
    "            lis = ul.find_all(\"li\")\n",
    "            for item in lis:\n",
    "                title = item.find(\"a\").text\n",
    "                press = item.find(\"span\", {\"class\" : \"writing\"}).text\n",
    "                \n",
    "                # link\n",
    "                link = item.find(\"a\")[\"href\"]\n",
    "\n",
    "                sid1 = link.split('&')[-3].split('=')[1]\n",
    "                oid = link.split('&')[-2].split('=')[1]\n",
    "                aid = link.split('&')[-1].split('=')[1]\n",
    "                \n",
    "                # 연예 TV 기사 제외\n",
    "                if sid1 == \"shm\":\n",
    "                    continue\n",
    "\n",
    "                article_type = \"text\"\n",
    "                \n",
    "                text_list = [date, article_type, title, press, sid1, link, aid]\n",
    "                \n",
    "                main_news_list.append(text_list)\n",
    "\n",
    "\n",
    "# make .csv file\n",
    "naver_news_main_df = pd.DataFrame(main_news_list, \n",
    "                                  columns = [\"date\", \"type\", \"title\", \"press\", \"category\", \"link\", \"aid\"])\n",
    "naver_news_main_df.to_csv(\"naver_main_news_{}_to_{}.csv\".format(days_range[0], days_range[-1]), index=False)\n",
    "\n",
    "print(\"=== total # of articles is {} ===\".format(len(main_news_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d364648b08143bab6d4d053ee8a4dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2538e22fda48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# 포토 뉴스 페이지 수 구하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mphoto_news_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"eh_page\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mphoto_news_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphoto_news_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from pprint import pprint\n",
    "\n",
    "test = [\"2015-03-01\"]\n",
    "main_news_list = []\n",
    "\n",
    "for date in tqdm_notebook(test):\n",
    "    \n",
    "    news_arrange_url = \"https://news.naver.com/main/history/mainnews/list.nhn\"\n",
    "    news_list_date_page_url = news_arrange_url + \"?date=\" + date\n",
    "\n",
    "    # get bs_obj\n",
    "    bs_obj = get_bs_obj(news_list_date_page_url)\n",
    "    \n",
    "    # 포토 뉴스 페이지 수 구하기\n",
    "    photo_news_count = bs_obj.find(\"div\", {\"class\": \"eh_page\"}).text.split('/')[1]\n",
    "    photo_news_count = int(photo_news_count)\n",
    "    \n",
    "    # 리스트 뉴스 페이지 수 구하기\n",
    "    text_news_count = bs_obj.find(\"div\", {\"class\": \"mtype_list_wide\"}).find(\"div\", {\"class\": \"eh_page\"}).text.split('/')[1]\n",
    "    text_news_count = int(text_news_count)\n",
    "    \n",
    "    # 포토 뉴스 부분 링크 크롤링\n",
    "    for page in tqdm_notebook(range(1,photo_news_count+1)):\n",
    "        \n",
    "        # 포토 뉴스 링크\n",
    "        news_list_photo_url = 'http://news.naver.com/main/history/mainnews/photoTv.nhn'\n",
    "        date_str = \"?date=\"\n",
    "        page_str = \"&page=\"\n",
    "        news_list_photo_full_url = news_list_photo_url + \"?date=\" + date + \"&page=\" + str(page)\n",
    "        \n",
    "        # get bs obj\n",
    "        photo_bs_obj = get_bs_obj(news_list_photo_full_url)\n",
    "        \n",
    "        # 링크 내 정보 수집\n",
    "        ul = photo_bs_obj.find(\"ul\", {\"class\": \"edit_history_lst\"})\n",
    "        lis = ul.find_all(\"li\")\n",
    "        for item in lis:\n",
    "            title = item.find(\"a\")[\"title\"]\n",
    "            press = item.find(\"span\", {\"class\" : \"eh_by\"}).text\n",
    "            \n",
    "            # link\n",
    "            link = item.find(\"a\")[\"href\"]\n",
    "            \n",
    "            sid1 = link.split('&')[-3].split('=')[1]\n",
    "            oid = link.split('&')[-2].split('=')[1]\n",
    "            aid = link.split('&')[-1].split('=')[1]            \n",
    "            \n",
    "            # 연예 TV 기사 제외\n",
    "            if sid1 == \"shm\":\n",
    "                continue\n",
    "                \n",
    "            article_type = \"pic\"\n",
    "            \n",
    "            pic_list = [date, article_type, title, press, sid1, link, aid]\n",
    "            \n",
    "            main_news_list.append(pic_list)\n",
    "            \n",
    "    # 텍스트 뉴스 부분 링크 크롤링\n",
    "    for page in tqdm_notebook(range(1, text_news_count+1)):\n",
    "        \n",
    "        # 텍스트 뉴스 링크\n",
    "        news_list_text_url = 'http://news.naver.com/main/history/mainnews/text.nhn'\n",
    "        date_str = \"?date=\"\n",
    "        page_str = \"&page=\"\n",
    "        news_list_text_full_url = news_list_text_url + \"?date=\" + date + \"&page=\" + str(page)\n",
    "        \n",
    "        # get bs obj\n",
    "        text_bs_obj = get_bs_obj(news_list_text_full_url)\n",
    "        \n",
    "        # 링크 내 정보 수집\n",
    "        uls = text_bs_obj.find_all(\"ul\")\n",
    "        for ul in uls:\n",
    "            lis = ul.find_all(\"li\")\n",
    "            for item in lis:\n",
    "                title = item.find(\"a\").text\n",
    "                press = item.find(\"span\", {\"class\" : \"writing\"}).text\n",
    "                \n",
    "                # link\n",
    "                link = item.find(\"a\")[\"href\"]\n",
    "\n",
    "                sid1 = link.split('&')[-3].split('=')[1]\n",
    "                oid = link.split('&')[-2].split('=')[1]\n",
    "                aid = link.split('&')[-1].split('=')[1]\n",
    "                \n",
    "                # 연예 TV 기사 제외\n",
    "                if sid1 == \"shm\":\n",
    "                    continue\n",
    "\n",
    "                article_type = \"text\"\n",
    "                \n",
    "                text_list = [date, article_type, title, press, sid1, link, aid]\n",
    "                \n",
    "                main_news_list.append(text_list)\n",
    "\n",
    "pprint(main_news_list, width = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
